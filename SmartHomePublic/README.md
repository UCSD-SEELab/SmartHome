# SEELab SmartHome Repository

Welcome to the SEELab SmartHome repository!

## Repository Structure

The repository is organized into the following sub-directories:

```
  /code
    /analysis - Code to run comparison of hierarchy aware and monolithic inference
    /cleaning - Code to process raw data files and extract features
    /lib      - TensorFlow implementation of hierarchical neural network
    /utils    - Utility Python code used for cleaning and analysis 
    
  /data - Contains raw data files extracted from SmartHome deployment
  
  /output - Contains persistent output produced by code in `analysis`. 
            TensorFlow saved model files are written here.
```

## Quick and Easy Feature Extraction

To generate a dataset for analysis perform the following steps:

```
# (1) Clone this repository
git clone <URL>/SmartHomePublic.git
cd SmartHomePublic

# (2) Install necessary python packages
pip install -r requirements.txt

# (3) Run code to parse raw data and extract features
cd SmartHomePublic/code/cleaning
python run.py
```

The output of this process will be an HDF5 file containing two Pandas data frames which can be used in analysis algorithms. For detailed examples of how to use these files inspect `/code/analysis/mlp.py`. For detailed documentation on Pandas see: [Pandas Tutorials](https://pandas.pydata.org/pandas-docs/stable/tutorials.html) A simple example is given below:

```
import pandas as pd
processed_data = pd.read_hdf("../../temp/data_processed.h5", "subject1")
processed_data.describe()  # Print a summary of the dataset
```

In the following three sections we document in greater detail the steps performed to extract raw data and perform analysis.

## Data Description

**Note: we provide data parsing scripts. This section merely provides documentation for completeness**. We provide output from three rounds of data collection performed on three different days in the same location. Raw data files are contained in the `/data` directory. The files are named: `MQTT_Messages_subject<number>_<date>.txt`. Each row in a file corresponds to one reading from one sensor. An example row is described below:

`Topic:#pir/angular_locations#Message:#{"posy": 19.0, "angle1": 45.0, "posx": 6.0, "angle2": 45.0}`

`Topic:#` indcates the sensor corresponding to this data item. In the example above, the sensor is the `pir/angular_locations` sensor. `Message:#` is the JSON object containing the data generated by the sensor. There are two data stream which do not conform to this format:

1) **Smart Watch** The data fields produced by the smart watch a `;` delimited:

`Topic:#watch#Message:#step;9047.0;hrm;137.0;acc;-6.23;-0.531;6.601;gyro;-0.947;0.29;0.402;ts;11/16 13:37:45:879`

2) **Smart Things** The smart things data streams return only a single value:

`Topic:#smartthings/Fridge/temperature#Message:#70`

In this example the `smartthings/Fridge/temperature` topic produces the message `70`

## Data Parsing

Code to parse the raw data files is contained in `/code/cleaning`. Data parsing is performed by two code files:

1. `read_data.py`: This file reads the raw `.txt` data files and converts each sensor data stream into a Python data structure. The output is a dictionary whose keys are the names of data streams (topics) and whose values are lists containing the corresponding raw data. Little to no data processing is performed in this stage. The timestamp of sensor messages are aligned with the timestamp of the smart-watch which samples at the highest rate.
2. `preclean.py`: This file takes the output of (1) and converts each data stream into a Pandas data frame. Data frames are saved into an HDF5 file (requires `pytables`) organized by topic name.

## Feature Extraction

Code to extract features from data is contained in `/code/cleaning/build.py`. **Before running this program you must run preclean.py which generates input data**

## Analysis

## General Comments
